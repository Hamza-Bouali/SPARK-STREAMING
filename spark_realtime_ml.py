from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
import os
import json
from datetime import datetime

# Kafka connection (Docker)
kafka_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")

# Create Spark session with compatible Kafka connector for PySpark 4.1.0
spark = SparkSession.builder \
    .appName("BitcoinRealtimeML") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0") \
    .config("spark.sql.streaming.checkpointLocation", "./checkpoint") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print(f"üöÄ Starting Bitcoin Real-time ML Pipeline... Connecting to Kafka at {kafka_servers}")

# JSON Schema (removed marketcap)
schema = StructType([
    StructField("date", StringType(), True),
    StructField("open", DoubleType(), True),
    StructField("high", DoubleType(), True),
    StructField("low", DoubleType(), True),
    StructField("close", DoubleType(), True),
    StructField("volume", DoubleType(), True)
])

# Read from Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_servers) \
    .option("subscribe", "bitcoin_prices") \
    .option("startingOffsets", "earliest") \
    .load()

# Parse JSON
json_df = df.selectExpr("CAST(value AS STRING) as json_value")
parsed_df = json_df.select(from_json(col("json_value"), schema).alias("data")).select("data.*")

# Clean data - remove nulls
clean_df = parsed_df.dropna()

print("‚úÖ Data cleaning applied")

# Prepare features for ML
feature_cols = ["open", "high", "low", "volume"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# Helper function to load/save metrics history
def load_metrics_history():
    """Load existing metrics history"""
    history_file = "./metrics_history.json"
    try:
        if os.path.exists(history_file):
            with open(history_file, 'r') as f:
                return json.load(f)
    except:
        pass
    return []

def save_metrics_history(history):
    """Save metrics history to JSON"""
    try:
        with open("./metrics_history.json", "w") as f:
            json.dump(history, f, indent=2)
    except Exception as e:
        print(f"‚ö†Ô∏è  Error saving metrics history: {e}")

# Function to train model on each batch
def train_and_save_model(batch_df, batch_id):
    if batch_df.count() == 0:
        print(f"‚ö†Ô∏è  Batch {batch_id}: No data")
        return
    
    print(f"\nüìä Processing Batch {batch_id} with {batch_df.count()} records")
    
    # Prepare features
    data = assembler.transform(batch_df)
    final_data = data.select("features", col("close").alias("label"))
    
    # Split data
    train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)
    
    if train_data.count() < 2 or test_data.count() < 1:
        print(f"‚ö†Ô∏è  Batch {batch_id}: Not enough data for training")
        return
    
    # Train Gradient Boosting Regressor model
    gbt = GBTRegressor(
        labelCol="label",
        featuresCol="features",
        maxDepth=5,                    # Depth of trees
        maxBins=32,                    # Number of bins for feature discretization
        maxIter=20,                    # Number of boosting iterations (trees)
        stepSize=0.1,                  # Learning rate
        seed=42
    )
    model = gbt.fit(train_data)
    
    # Evaluate on test data
    predictions = model.transform(test_data)
    evaluator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
    rmse = evaluator.evaluate(predictions)
    
    # Also calculate MAE for better insight
    mae_evaluator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="mae")
    mae = mae_evaluator.evaluate(predictions)
    
    print(f"‚úÖ Batch {batch_id} - GBT Model trained!")
    print(f"   üìà RMSE: {rmse:.2f}")
    print(f"   üìä MAE: {mae:.2f}")
    print(f"   üå≥ Number of iterations: 20")
    print(f"   üìã Feature importances: {[f'{v:.4f}' for v in model.featureImportances.toArray()]}")
    
    # Save model (overwrite with latest)
    model_path = "./bitcoin_model"
    try:
        model.write().overwrite().save(model_path)
        print(f"   üíæ Model saved to {model_path}")
        
        # Get feature importances as a list
        feature_importances = model.featureImportances.toArray().tolist()
        
        # Also save metadata
        with open("./model_metrics.txt", "w") as f:
            f.write(f"Model: GradientBoosting Regressor\n")
            f.write(f"RMSE: {rmse}\n")
            f.write(f"MAE: {mae}\n")
            f.write(f"Trees: 20\n")
            f.write(f"Max Depth: 5\n")
            f.write(f"Feature Importances: {feature_importances}\n")
            f.write(f"Batch: {batch_id}\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
        
        # Save to metrics history for visualization
        history = load_metrics_history()
        history.append({
            "batch": batch_id,
            "rmse": float(rmse),
            "mae": float(mae),
            "trees": 20,
            "feature_importances": feature_importances,
            "timestamp": datetime.now().isoformat()
        })
        save_metrics_history(history)
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error saving model: {e}")

# Write stream with foreachBatch to train model
query = clean_df.writeStream \
    .foreachBatch(train_and_save_model) \
    .outputMode("update") \
    .start()

print("‚úÖ Streaming pipeline started!")
print("üì° Listening to Kafka topic 'bitcoin_prices'...")
print("ü§ñ Training model on each batch...")

query.awaitTermination()
