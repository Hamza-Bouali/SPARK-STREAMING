{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c645f7",
   "metadata": {},
   "source": [
    "# ğŸš€ Bitcoin Real-Time Analytics Live Demo\n",
    "\n",
    "## Comprehensive showcase of the entire data pipeline\n",
    "\n",
    "**âš ï¸ Known Issue & Fix:**\n",
    "- **Problem**: The API shows `model_loaded: false` because Spark hasn't trained the model yet\n",
    "- **Root Cause**: Producer sends all data at once then exits, Spark needs continuous streaming\n",
    "- **Solution**: Run the cells below to verify the pipeline and manually trigger model training\n",
    "\n",
    "This notebook demonstrates:\n",
    "- âœ… Docker services status\n",
    "- âœ… Live Kafka messages  \n",
    "- âœ… ML model training metrics\n",
    "- âœ… API predictions\n",
    "- âœ… Real-time data visualizations\n",
    "- âœ… Troubleshooting common issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661584a",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Check Docker Services Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb2edab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Docker Services Status\n",
      "==================================================\n",
      "NAME                 IMAGE                                        COMMAND                  SERVICE           CREATED         STATUS                   PORTS\n",
      "bitcoin-api          bitcoin-realtime-analytics-api               \"uvicorn api_serviceâ€¦\"   api               7 minutes ago   Up 6 minutes             0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp\n",
      "kafka                confluentinc/cp-kafka:7.5.0                  \"/etc/confluent/dockâ€¦\"   kafka             7 minutes ago   Up 7 minutes (healthy)   0.0.0.0:9092-9093->9092-9093/tcp, [::]:9092-9093->9092-9093/tcp\n",
      "kafka-ui             provectuslabs/kafka-ui:latest                \"/bin/sh -c 'java --â€¦\"   kafka-ui          7 minutes ago   Up 7 minutes             0.0.0.0:8080->8080/tcp, [::]:8080->8080/tcp\n",
      "realtime-dashboard   bitcoin-realtime-analytics-dashboard         \"streamlit run realtâ€¦\"   dashboard         7 minutes ago   Up 6 minutes             0.0.0.0:8501->8501/tcp, [::]:8501->8501/tcp\n",
      "spark-streaming      bitcoin-realtime-analytics-spark-streaming   \"python spark_realtiâ€¦\"   spark-streaming   7 minutes ago   Up 6 minutes             \n",
      "zookeeper            confluentinc/cp-zookeeper:7.5.0              \"/etc/confluent/dockâ€¦\"   zookeeper         7 minutes ago   Up 7 minutes             2888/tcp, 0.0.0.0:2181->2181/tcp, [::]:2181->2181/tcp, 3888/tcp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Execute shell command and return output\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout + result.stderr\n",
    "\n",
    "# Check Docker services\n",
    "print(\"ğŸ“Š Docker Services Status\")\n",
    "print(\"=\" * 50)\n",
    "output = run_command(\"docker compose ps\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f906a21",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Monitor Kafka Producer (Data Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d033a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¨ Bitcoin Producer Activity (Last 20 lines)\n",
      "==================================================\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 05:00:00', 'open': 91349.1484375, 'high': 91438.0, 'low': 91317.9609375, 'close': 91432.5703125, 'volume': 0}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 06:00:00', 'open': 91434.1328125, 'high': 91685.1796875, 'low': 91213.8046875, 'close': 91213.8046875, 'volume': 101595136}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 07:00:00', 'open': 91206.6875, 'high': 91474.9375, 'low': 91206.6875, 'close': 91474.9375, 'volume': 42860544}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 08:00:00', 'open': 91487.390625, 'high': 91487.390625, 'low': 91176.1171875, 'close': 91307.09375, 'volume': 265007104}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 09:00:00', 'open': 91327.6875, 'high': 91426.203125, 'low': 91244.1015625, 'close': 91409.796875, 'volume': 814034944}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 10:00:00', 'open': 91418.1171875, 'high': 91554.625, 'low': 91387.0234375, 'close': 91538.21875, 'volume': 971368448}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 11:00:00', 'open': 91533.859375, 'high': 91533.859375, 'low': 91205.5, 'close': 91235.6796875, 'volume': 414734336}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 12:00:00', 'open': 91236.4609375, 'high': 91300.234375, 'low': 91078.1171875, 'close': 91098.34375, 'volume': 311578624}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 13:00:00', 'open': 91080.828125, 'high': 91223.3515625, 'low': 91015.5078125, 'close': 91223.3515625, 'volume': 114096128}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 14:00:00', 'open': 91212.140625, 'high': 91349.0859375, 'low': 91103.0546875, 'close': 91270.609375, 'volume': 5550080}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 15:00:00', 'open': 91274.171875, 'high': 91320.7109375, 'low': 91071.453125, 'close': 91270.453125, 'volume': 203657216}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 16:00:00', 'open': 91286.796875, 'high': 91391.921875, 'low': 91219.4765625, 'close': 91285.140625, 'volume': 283027456}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 17:00:00', 'open': 91282.5078125, 'high': 91391.234375, 'low': 91276.5, 'close': 91297.8203125, 'volume': 180326400}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 18:00:00', 'open': 91294.46875, 'high': 91294.46875, 'low': 90965.75, 'close': 91161.8359375, 'volume': 358440960}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 19:00:00', 'open': 91164.640625, 'high': 91164.640625, 'low': 90877.046875, 'close': 91099.46875, 'volume': 530702336}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 20:00:00', 'open': 91100.7890625, 'high': 91298.765625, 'low': 91084.6875, 'close': 91252.453125, 'volume': 601075712}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 21:00:00', 'open': 91259.6171875, 'high': 91350.375, 'low': 91147.78125, 'close': 91220.7734375, 'volume': 442963968}\n",
      "bitcoin-producer  | â¡ Sent: {'date': '2026-01-04 22:00:00', 'open': 91224.3984375, 'high': 91338.75, 'low': 91211.2734375, 'close': 91281.296875, 'volume': 32178176}\n",
      "bitcoin-producer  | \n",
      "bitcoin-producer  | âœ” All data has been sent to Kafka!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“¨ Bitcoin Producer Activity (Last 20 lines)\")\n",
    "print(\"=\" * 50)\n",
    "logs = run_command(\"docker compose logs producer | tail -20\")\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcc993",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Monitor Spark Streaming (ML Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccd8d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Spark Streaming Activity (Last 20 lines)\n",
      "==================================================\n",
      "spark-streaming  | \t---------------------------------------------------------------------\n",
      "spark-streaming  | \t|                  |            modules            ||   artifacts   |\n",
      "spark-streaming  | \t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "spark-streaming  | \t---------------------------------------------------------------------\n",
      "spark-streaming  | \t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
      "spark-streaming  | \t---------------------------------------------------------------------\n",
      "spark-streaming  | :: retrieving :: org.apache.spark#spark-submit-parent-1e60ec4a-9f3d-4d3d-93ee-614f20fbd7f0\n",
      "spark-streaming  | \tconfs: [default]\n",
      "spark-streaming  | \t11 artifacts copied, 0 already retrieved (62936kB/349ms)\n",
      "spark-streaming  | 26/01/04 22:11:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "spark-streaming  | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "spark-streaming  | Setting default log level to \"WARN\".\n",
      "spark-streaming  | To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "spark-streaming  | 26/01/04 22:11:31 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "spark-streaming  | \n",
      "[Stage 0:>                                                          (0 + 1) / 1]\n",
      "\n",
      "[Stage 0:===========================================================(1 + 0) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 6:>                                                          (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 13:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 14:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 16:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 26:>                                                         (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 216:>                                                        (0 + 1) / 1]\n",
      "26/01/04 22:12:19 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "spark-streaming  | \n",
      "                                                                                \n",
      "\n",
      "[Stage 217:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "26/01/04 22:12:21 WARN FileUtil: Failed to delete file or dir [/app/bitcoin_model]: it still exists.\n",
      "spark-streaming  | \n",
      "[Stage 221:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 225:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 228:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 231:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 234:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 238:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "26/01/04 22:12:48 WARN FileUtil: Failed to delete file or dir [/app/bitcoin_model]: it still exists.\n",
      "spark-streaming  | 26/01/04 22:13:04 WARN FileUtil: Failed to delete file or dir [/app/bitcoin_model]: it still exists.\n",
      "spark-streaming  | 26/01/04 22:13:17 WARN FileUtil: Failed to delete file or dir [/app/bitcoin_model]: it still exists.\n",
      "spark-streaming  | \n",
      "[Stage 903:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 909:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 912:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 913:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 914:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 916:>                                                        (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n",
      "26/01/04 22:13:32 WARN FileUtil: Failed to delete file or dir [/app/bitcoin_model]: it still exists.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¯ Spark Streaming Activity (Last 20 lines)\")\n",
    "print(\"=\" * 50)\n",
    "logs = run_command(\"docker compose logs spark-streaming | tail -20\")\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bba14e",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Check ML Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffd9cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– ML Model Training Metrics\n",
      "==================================================\n",
      "Model: GradientBoosting Regressor\n",
      "RMSE: 309.577502769582\n",
      "MAE: 201.99444570937837\n",
      "Trees: 20\n",
      "Max Depth: 5\n",
      "Feature Importances: [0.01866401420771625, 0.08788473715447968, 0.8827465191215687, 0.010704729516235311]\n",
      "Batch: 4\n",
      "Timestamp: 2026-01-04T22:13:33.326503\n",
      "\n",
      "\n",
      "âœ… Model metrics are being generated in real-time!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"ğŸ¤– ML Model Training Metrics\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "metrics_file = \"/home/azureuser/bitcoin_spark/bitcoin-realtime-analytics/model_metrics.txt\"\n",
    "\n",
    "if os.path.exists(metrics_file):\n",
    "    with open(metrics_file, 'r') as f:\n",
    "        metrics = f.read()\n",
    "    print(metrics)\n",
    "    print(\"\\nâœ… Model metrics are being generated in real-time!\")\n",
    "else:\n",
    "    print(\"â³ Metrics file still being generated by Spark Streaming...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2e20b",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Check Metrics History (JSON Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0434241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Model Performance History (Last 10 updates)\n",
      "================================================================================\n",
      "    batch        rmse         mae  trees                                                                        feature_importances                   timestamp\n",
      "37      0  986.639303  640.808466     20   [0.0006534188413004734, 0.08807232731935773, 0.9111622379338252, 0.00011201590551653004]  2026-01-04T14:51:23.564385\n",
      "38      1  696.441206  453.691230     20  [0.00028060492918761335, 0.950709227352462, 0.048824481457239624, 0.00018568626111092497]  2026-01-04T14:51:41.282262\n",
      "39      2  328.482755  211.402530     20       [0.005229023024365457, 0.7481584470175465, 0.23782596996101538, 0.00878655999707271]  2026-01-04T14:51:52.769603\n",
      "40      3  564.979630  381.917351     20    [0.0012024750061918597, 0.05168384275959068, 0.9446254348134879, 0.0024882474207294386]  2026-01-04T14:52:03.005870\n",
      "41      4  251.462887  171.003612     20      [0.014265010662428208, 0.07123713337155095, 0.9009093946233449, 0.013588461342676019]  2026-01-04T14:52:14.598393\n",
      "42      0  469.453825  297.949727     20     [0.0007147477946680425, 0.8381999124955115, 0.1597697454349989, 0.0013155942748213818]  2026-01-04T22:12:25.009287\n",
      "43      1  677.737864  455.272923     20   [0.0004983164092704692, 0.8404361464955459, 0.15867020928425107, 0.00039532781093250155]  2026-01-04T22:12:49.189619\n",
      "44      2  459.535773  271.924304     20    [0.0021479762795055626, 0.8830296864574828, 0.11103463103377682, 0.0037877062292348395]  2026-01-04T22:13:05.924919\n",
      "45      3  559.444444  340.391322     20     [0.004095563110170412, 0.7096235415438822, 0.28288950550089154, 0.0033913898450559344]  2026-01-04T22:13:18.441370\n",
      "46      4  309.577503  201.994446     20       [0.01866401420771625, 0.08788473715447968, 0.8827465191215687, 0.010704729516235311]  2026-01-04T22:13:33.327821\n",
      "\n",
      "âœ… Total training iterations: 47\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "metrics_history_file = \"/home/azureuser/bitcoin_spark/bitcoin-realtime-analytics/metrics_history.json\"\n",
    "\n",
    "if os.path.exists(metrics_history_file):\n",
    "    with open(metrics_history_file, 'r') as f:\n",
    "        try:\n",
    "            history = json.load(f)\n",
    "            \n",
    "            # Display as DataFrame\n",
    "            if isinstance(history, list) and len(history) > 0:\n",
    "                df_metrics = pd.DataFrame(history)\n",
    "                print(\"ğŸ“ˆ Model Performance History (Last 10 updates)\")\n",
    "                print(\"=\" * 80)\n",
    "                print(df_metrics.tail(10).to_string())\n",
    "                print(f\"\\nâœ… Total training iterations: {len(history)}\")\n",
    "            else:\n",
    "                print(\"â³ Metrics history is accumulating...\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"â³ Metrics file is still being written...\")\n",
    "else:\n",
    "    print(\"â³ Metrics history file not yet created...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bd7a7",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Test the REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9aeffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Testing REST API\n",
      "==================================================\n",
      "âœ… API Status: 200\n",
      "Response: {'message': 'Bitcoin Price Prediction API', 'status': 'running', 'model_loaded': False, 'endpoints': {'predict': '/predict', 'batch_predict': '/batch_predict', 'model_info': '/model/info', 'reload_model': '/model/reload', 'health': '/health'}}\n",
      "\n",
      "âš ï¸ Prediction endpoint returned: 405\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "print(\"ğŸŒ Testing REST API\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Test health endpoint\n",
    "    response = requests.get(f\"{API_URL}/\", timeout=5)\n",
    "    print(f\"âœ… API Status: {response.status_code}\")\n",
    "    print(f\"Response: {response.json()}\")\n",
    "    print()\n",
    "    \n",
    "    # Test predict endpoint\n",
    "    response = requests.get(f\"{API_URL}/predict\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()\n",
    "        print(\"ğŸ¯ Latest Prediction from API:\")\n",
    "        print(json.dumps(prediction, indent=2))\n",
    "    else:\n",
    "        print(f\"âš ï¸ Prediction endpoint returned: {response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"â³ API not yet available. Please wait for services to start...\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ API Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bab99f",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ View Real-Time Data Pipeline Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9cd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Real-Time Pipeline Statistics\n",
      "==================================================\n",
      "ğŸ“¨ Producer:\n",
      "bitcoin-producer  | âœ… Successfully connected to Kafka at kafka:29092\n",
      "bitcoin-producer  |  Total records to send: 17511\n",
      "\n",
      "\n",
      "ğŸ¯ Spark ML:\n",
      "â³ No data yet...\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Real-Time Pipeline Statistics\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get producer statistics\n",
    "producer_logs = run_command(\"docker compose logs producer | grep -E '(Total records|Successfully)' | head -5\")\n",
    "print(\"ğŸ“¨ Producer:\")\n",
    "print(producer_logs if producer_logs else \"â³ No data yet...\")\n",
    "print()\n",
    "\n",
    "# Get Spark statistics\n",
    "spark_logs = run_command(\"docker compose logs spark-streaming | grep -E '(records|training|RMSE|MAE)' | tail -5\")\n",
    "print(\"ğŸ¯ Spark ML:\")\n",
    "print(spark_logs if spark_logs else \"â³ No data yet...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720498a",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Quick Access Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cefe527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ OPEN THESE IN YOUR BROWSER:\n",
      "==================================================\n",
      "\n",
      "1. ğŸ“Š Kafka UI (View live messages):\n",
      "   http://localhost:8080\n",
      "\n",
      "2. ğŸ“ˆ Real-Time Dashboard:\n",
      "   http://localhost:8501\n",
      "\n",
      "3. ğŸ”§ API Documentation (Interactive):\n",
      "   http://localhost:8000/docs\n",
      "\n",
      "4. ğŸ“‹ OpenAPI Schema:\n",
      "   http://localhost:8000/openapi.json\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸŒ OPEN THESE IN YOUR BROWSER:\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"1. ğŸ“Š Kafka UI (View live messages):\")\n",
    "print(\"   http://localhost:8080\")\n",
    "print()\n",
    "print(\"2. ğŸ“ˆ Real-Time Dashboard:\")\n",
    "print(\"   http://localhost:8501\")\n",
    "print()\n",
    "print(\"3. ğŸ”§ API Documentation (Interactive):\")\n",
    "print(\"   http://localhost:8000/docs\")\n",
    "print()\n",
    "print(\"4. ğŸ“‹ OpenAPI Schema:\")\n",
    "print(\"   http://localhost:8000/openapi.json\")\n",
    "print()\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d584c2",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ System Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42709203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘           BITCOIN REAL-TIME ANALYTICS ARCHITECTURE                 â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 1. DATA INGESTION (bitcoin_producer.py)                             â”‚\n",
      "â”‚    â”œâ”€ Downloads BTC-USD data from yfinance                          â”‚\n",
      "â”‚    â”œâ”€ Publishes to Kafka topic: bitcoin_data                        â”‚\n",
      "â”‚    â””â”€ Runs every interval (default: hourly)                         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "           â”‚\n",
      "           â–¼\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 2. MESSAGE BROKER (Apache Kafka)                                    â”‚\n",
      "â”‚    â”œâ”€ Zookeeper: localhost:2181                                     â”‚\n",
      "â”‚    â”œâ”€ Kafka: localhost:9092 (external) / kafka:29092 (internal)     â”‚\n",
      "â”‚    â””â”€ Kafka UI: http://localhost:8080                               â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "           â”‚\n",
      "           â–¼\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 3. STREAM PROCESSING (spark_realtime_ml.py)                         â”‚\n",
      "â”‚    â”œâ”€ Reads from Kafka topics                                       â”‚\n",
      "â”‚    â”œâ”€ Random Forest ML model training                               â”‚\n",
      "â”‚    â”œâ”€ Real-time prediction generation                               â”‚\n",
      "â”‚    â””â”€ Metrics: RMSE, MAE, RÂ²                                        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "           â”‚\n",
      "           â–¼\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ 4. OUTPUT LAYER                                                     â”‚\n",
      "â”‚    â”œâ”€ API (FastAPI): http://localhost:8000/docs                    â”‚\n",
      "â”‚    â”œâ”€ Dashboard (Streamlit): http://localhost:8501                 â”‚\n",
      "â”‚    â””â”€ Metrics Files: model_metrics.txt, metrics_history.json        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "âœ… All components are containerized and orchestrated with Docker Compose\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           BITCOIN REAL-TIME ANALYTICS ARCHITECTURE                 â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. DATA INGESTION (bitcoin_producer.py)                             â”‚\n",
    "â”‚    â”œâ”€ Downloads BTC-USD data from yfinance                          â”‚\n",
    "â”‚    â”œâ”€ Publishes to Kafka topic: bitcoin_data                        â”‚\n",
    "â”‚    â””â”€ Runs every interval (default: hourly)                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 2. MESSAGE BROKER (Apache Kafka)                                    â”‚\n",
    "â”‚    â”œâ”€ Zookeeper: localhost:2181                                     â”‚\n",
    "â”‚    â”œâ”€ Kafka: localhost:9092 (external) / kafka:29092 (internal)     â”‚\n",
    "â”‚    â””â”€ Kafka UI: http://localhost:8080                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 3. STREAM PROCESSING (spark_realtime_ml.py)                         â”‚\n",
    "â”‚    â”œâ”€ Reads from Kafka topics                                       â”‚\n",
    "â”‚    â”œâ”€ Random Forest ML model training                               â”‚\n",
    "â”‚    â”œâ”€ Real-time prediction generation                               â”‚\n",
    "â”‚    â””â”€ Metrics: RMSE, MAE, RÂ²                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 4. OUTPUT LAYER                                                     â”‚\n",
    "â”‚    â”œâ”€ API (FastAPI): http://localhost:8000/docs                    â”‚\n",
    "â”‚    â”œâ”€ Dashboard (Streamlit): http://localhost:8501                 â”‚\n",
    "â”‚    â””â”€ Metrics Files: model_metrics.txt, metrics_history.json        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "âœ… All components are containerized and orchestrated with Docker Compose\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d4d03",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ Troubleshooting & Status Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caab224",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ Model Loading Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "966643a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Full System Health Check\n",
      "==================================================\n",
      "\n",
      "1. Docker Services:\n",
      "<no value>           STATUS\n",
      "bitcoin-api          Up 17 seconds\n",
      "kafka                Up 11 minutes (healthy)\n",
      "kafka-ui             Up 11 minutes\n",
      "realtime-dashboard   Up 11 minutes\n",
      "spark-streaming      Up 11 minutes\n",
      "zookeeper            Up 11 minutes\n",
      "\n",
      "\n",
      "2. Network Connectivity:\n",
      "Kafka connectivity: â³ Starting...\n",
      "\n",
      "3. Generated Files:\n",
      "   âœ… model_metrics.txt: 0.25 KB\n",
      "   âœ… metrics_history.json: 13.54 KB\n",
      "   âœ… checkpoint/: 91 items\n",
      "\n",
      "==================================================\n",
      "ğŸ’¡ If services are not ready, wait 30-60 seconds and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ Full System Health Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Docker status\n",
    "print(\"\\n1. Docker Services:\")\n",
    "status = run_command(\"docker compose ps --format 'table {{.Names}}\\\\t{{.Status}}'\")\n",
    "print(status)\n",
    "\n",
    "# Network check\n",
    "print(\"\\n2. Network Connectivity:\")\n",
    "kafka_check = run_command(\"docker compose exec -T kafka kafka-broker-api-versions --bootstrap-server localhost:29092 2>&1 | head -3\")\n",
    "print(f\"Kafka connectivity: {'âœ…' if 'ApiVersion' in kafka_check else 'â³ Starting...'}\")\n",
    "\n",
    "# File system check\n",
    "print(\"\\n3. Generated Files:\")\n",
    "files_to_check = [\n",
    "    \"model_metrics.txt\",\n",
    "    \"metrics_history.json\",\n",
    "    \"checkpoint/\"\n",
    "]\n",
    "\n",
    "base_path = \"/home/azureuser/bitcoin_spark/bitcoin-realtime-analytics\"\n",
    "for file in files_to_check:\n",
    "    full_path = os.path.join(base_path, file)\n",
    "    if os.path.exists(full_path):\n",
    "        if os.path.isdir(full_path):\n",
    "            count = len(os.listdir(full_path))\n",
    "            print(f\"   âœ… {file}: {count} items\")\n",
    "        else:\n",
    "            size = os.path.getsize(full_path) / 1024\n",
    "            print(f\"   âœ… {file}: {size:.2f} KB\")\n",
    "    else:\n",
    "        print(f\"   â³ {file}: Pending...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ’¡ If services are not ready, wait 30-60 seconds and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Model Loading Troubleshooting\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "base_path = \"/home/azureuser/bitcoin_spark/bitcoin-realtime-analytics\"\n",
    "\n",
    "# Check if model exists\n",
    "model_path = os.path.join(base_path, \"bitcoin_model\")\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"\\nâœ… Model directory exists: {model_path}\")\n",
    "    \n",
    "    # Check model contents\n",
    "    model_contents = os.listdir(model_path)\n",
    "    print(f\"   Contents: {', '.join(model_contents)}\")\n",
    "    \n",
    "    # Check model type\n",
    "    metadata_path = os.path.join(model_path, \"metadata\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        print(f\"\\nğŸ“‹ Model Type: Gradient Boosting Tree (GBT)\")\n",
    "        print(f\"   Structure:\")\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            level = root.replace(model_path, '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f'{indent}{os.path.basename(root)}/')\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files[:3]:  # Show first 3 files per dir\n",
    "                print(f'{subindent}{file}')\n",
    "            if len(files) > 3:\n",
    "                print(f'{subindent}... and {len(files) - 3} more files')\n",
    "else:\n",
    "    print(f\"\\nâŒ Model directory NOT found: {model_path}\")\n",
    "    print(\"\\nğŸ’¡ To fix:\")\n",
    "    print(\"   1. Ensure Spark streaming is running and has received data\")\n",
    "    print(\"   2. Check: docker compose logs spark-streaming\")\n",
    "    print(\"   3. Producer must send data to 'bitcoin_prices' topic\")\n",
    "\n",
    "# Check Spark streaming status\n",
    "print(\"\\n\\nğŸ“Š Spark Streaming Status:\")\n",
    "spark_logs = run_command(\"docker compose logs spark-streaming 2>&1 | grep -E '(ğŸ“Š|âœ…|Batch|Training)' | tail -10\")\n",
    "if spark_logs.strip():\n",
    "    print(spark_logs)\n",
    "else:\n",
    "    print(\"   âš ï¸ No training activity detected\")\n",
    "    print(\"   ğŸ’¡ Spark may still be initializing or waiting for data\")\n",
    "\n",
    "# Test API model reload\n",
    "print(\"\\n\\nğŸ”„ Attempting to reload model via API...\")\n",
    "try:\n",
    "    response = requests.post(\"http://localhost:8000/model/reload\", timeout=5)\n",
    "    result = response.json()\n",
    "    print(f\"   Result: {json.dumps(result, indent=2)}\")\n",
    "    \n",
    "    if not result.get(\"success\"):\n",
    "        print(\"\\nâŒ Model reload failed\")\n",
    "        print(\"\\nğŸ”§ Quick Fix:\")\n",
    "        print(\"   1. Wait 2-3 minutes for Spark to finish training\")\n",
    "        print(\"   2. Re-run this cell\")\n",
    "        print(\"   3. If still failing, restart: docker compose restart spark-streaming\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ API Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958adef8",
   "metadata": {},
   "source": [
    "## âœ¨ Summary\n",
    "\n",
    "This notebook provides a comprehensive live demonstration of the Bitcoin Real-Time Analytics project:\n",
    "\n",
    "âœ… **Data Pipeline**: Real Bitcoin data â†’ Kafka â†’ Spark ML â†’ Predictions  \n",
    "âœ… **Services**: All containerized and auto-starting  \n",
    "âœ… **Monitoring**: Live logs and metrics tracking  \n",
    "âœ… **APIs**: RESTful API with OpenAPI docs  \n",
    "âœ… **Visualization**: Interactive dashboard and Kafka UI  \n",
    "\n",
    "### Next Steps:\n",
    "1. **Re-run all cells** to refresh the status\n",
    "2. **Open the browser links** to see live data flowing\n",
    "3. **Check metrics** improving as more data arrives\n",
    "4. **Test API** with custom parameters via the swagger UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
